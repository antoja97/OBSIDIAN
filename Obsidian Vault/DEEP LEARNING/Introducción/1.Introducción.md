Diferencia con el ML es que este se usa para datos tabulares, mientras que el DL se aplica a audio, imagen, etc.

*Redes Neuronales*: Tipos de Aprendizaje
La red neuronal funciona con cualquier tipo de problema y normalmente se usa para aprendizaje supervisado y por refuerzo.

### Tensores
![[Captura de pantalla 2023-05-19 a las 10.56.44.png]]

#### Herramientas

Keras, TensorFlow y PyTorch.


#### Inspiración biológica

Esto se debe a que las neuronas recogen información de otras neuronas, las cuales producen cambios en el cuerpo, mediante pequeñas señales, las cuales si superan cierto umbral, se propaga a las siguientes neuronas a través del axón.

#### *McCulloch-Pitts neuron*
Los componentes de la neurona son:
-Inputs
-Pesos
-Problemas Binarios
-Bias
-Activation function: si la señal cumple cierto threshold, se activa la salida.

#### Perceptrón
A diferencia de una neurona o una puerta lógica, aprende automaticamente los pesos para minimizar los errores

Neurona + estimador = Perceptrón (Reg Lin (sin activación) o Reg Log (con activación))

¿Cómo se entrena?

![[Captura de pantalla 2023-05-19 a las 11.51.41.png]]

ANN (Redes Neuronales Artificiales)

Se utiliza una serie de neuronas artificiales o nodos para solucionar problemas (muchos perceptrones juntos) (MULTILAYER PERCEPTRON)

**** Componentes MLP***
![[Captura de pantalla 2023-05-19 a las 11.57.50.png]]

#### Funciones de activación

Permiten a la red neuronal aprender patrones no lineares y adaptarse a problemas más complejos.
La más utilizada es Relu(Rectified linear) ,Sigmoid o soft max (que es como la sigmoide pero con más componentes)


## Backpropagation
Algoritmo de entrenamiento de una red neuronal.
Tras la forwardpropagation, la predicción se compara con el true label y obtenemos un error. 
Este error se propaga hacia atras para corregir los valores de los pesos de las funciones de transformación de las neuronas.

*Early stopping*
Podemos sufrir de overfitting, donde además las redes neuronales son muy sensibles a esto, ya que con el Backpropagation se van actualizando los pesos hasta minimizar el error.
Este proceso puede noa cabar nunca.

Las iteraciones hacia atras que realiza el algoritmo se denominan ***epochs .
El early sttoping permite parar el entrenamiento cuando empiezan a empeorar los resultados en validación. En ese momento habremos encontrado el punto óptimo de bias vs variance.****

#### Optimización
Diferentes funciones de optimizaión, cada una con diferentes maneras de llegar al punto mínimo (el más usado es el Mini-Batch Gradient Descent):

![[Captura de pantalla 2023-05-19 a las 13.40.49.png]]

#### Regularización
Sirven también para decur el overfitting.
Un ejemplo puede ser el dropout, que consiste en eliminar algunas neuronas aleatoriamente en el proceso de entrenamiento.
Las neuronas se van "apagando y encendiendo" aleatoriamente en cada step del algoritmo de entrenamiento.